{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import time\n",
    "import json\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from typing import Dict, List\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from fake_headers import Headers\n",
    "from bs4 import BeautifulSoup\n",
    "from returns.io import IO\n",
    "from lxml import html"
   ],
   "execution_count": 4,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "Общий алгоритм работы\n",
    "---------------------\n",
    "-> Определь неизменяемые входные данные ({url, domain, headers{browser,os,headers}}).\n",
    "-> Получить общее количество страниц пагинации с данными (товарами)\n",
    "-> Получить url всех страниц пагинации (в случае e-katalog это порядковый номер страницы)\n",
    "-> Получить список списков со всеми url конкретного товарова со всех страниц.\n",
    "-> Распаковать все списки в один список.\n",
    "-> Добавить ко всем url домен, как итог список окончательных ссылок к товару.\n",
    "_____________________\n",
    "'''"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nОбщий алгоритм работы\\n---------------------\\n-> Определь неизменяемые входные данные ({url, domain, headers{browser,os,headers}}).\\n-> Получить общее количество страниц пагинации с данными (товарами)\\n-> Получить url всех страниц пагинации (в случае e-katalog это порядковый номер страницы)\\n-> Получить список списков со всеми url конкретного товарова со всех страниц.\\n-> Распаковать все списки в один список.\\n-> Добавить ко всем url домен, как итог список окончательных ссылок к товару.\\n_____________________\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class imdict(dict):\n",
    "    def __hash__(self):\n",
    "        return id(self)\n",
    "\n",
    "    def _immutable(self, *args, **kws):\n",
    "        raise TypeError('object is immutable')\n",
    "\n",
    "    __setitem__ = _immutable\n",
    "    __delitem__ = _immutable\n",
    "    clear       = _immutable\n",
    "    update      = _immutable\n",
    "    setdefault  = _immutable\n",
    "    pop         = _immutable\n",
    "    popitem     = _immutable"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data = imdict({'URL':'https://www.e-katalog.ru/list/61/',\n",
    "               'DOMAIN': 'https://www.e-katalog.ru'})\n",
    "\n",
    "headers =imdict({'browser':\"chrome\",\n",
    "                 'os':\"win\",\n",
    "                 'headers': True})"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_headers(headers_setting: Dict) -> Dict:\n",
    "    '''\n",
    "    Генерирует html заголовки запросов. В данном случае фейковые.\n",
    "    '''\n",
    "    #TODO Расширить количество аргументов.\n",
    "\n",
    "    return Headers(\n",
    "                   browser = headers_setting['browser'],\n",
    "                   os = headers_setting['os'],\n",
    "                   headers = headers_setting['headers']\n",
    "                   ).generate()"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def get_page_html(url: str, verify: bool = False, **kwargs) -> BeautifulSoup:\n",
    "    '''На основе url получает всю html страницы, без потверждения ssl(опционально)'''\n",
    "\n",
    "    time.sleep(1)\n",
    "    return BeautifulSoup(requests.get(url, verify=verify, **kwargs).content, 'html.parser')"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def get_product_amount_page(url: str, headers: Dict = generate_headers(headers)) -> IO[int]:\n",
    "  \n",
    "    '''\n",
    "    Возвращает общее количество страниц для конкретного типа продукта\n",
    "    | ---\n",
    "    | url = базовый url страницы конкретного продукта (data['url'])\n",
    "    | headers = Шапка html запроса в формате словаря.\n",
    "    | ---\n",
    "    '''\n",
    "\n",
    "    def page_count(tree):\n",
    "        return tree.xpath('//div[@class=\"ib page-num\"]//a[last()]/text()')   \n",
    "\n",
    "    return int(page_count(html.fromstring(requests.get(url, headers = headers).content))[0])"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def collect_product_pages(url: str, pages_count: int) -> List[str]:\n",
    "    '''\n",
    "    *Получает список url страниц с товарами.*\n",
    "    | ---\n",
    "    | url = базовый url страницы конкретного продукта (data['url'])\n",
    "    | pages_count = Общее количество страниц для продукта.\n",
    "    | ---\n",
    "    '''\n",
    "    return list(map(lambda x: url + f\"{x}/\", range(1, pages_count+1)))"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def collect_url_from_page(page_url: str, headers = generate_headers(headers), sleep: int = 1) -> IO[List[str]]:\n",
    "    '''\n",
    "    *Получает все url продуктов со страницы(получает список страницы), возвращает список url*\n",
    "    | ---\n",
    "    | page_url = url страницы со товармаи\n",
    "    | headers = Шапка html запроса в формате словаря\n",
    "    | sleep = Время задержки после запроса\n",
    "    | ---\n",
    "    '''\n",
    "\n",
    "    def get_product_url(tree):\n",
    "        return tree.xpath(\"//a[@class='model-short-title no-u no-u']/@href\")\n",
    "    \n",
    "    time.sleep(sleep)\n",
    "    return get_product_url(html.fromstring(requests.get(page_url, headers=headers).content))\n",
    "    "
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def get_product_data(soup: BeautifulSoup) -> IO[Dict[str, str]]:\n",
    "    '''Получает BeautifulSoup объект. Получает имя товара, и таблицу с характеристиками'''\n",
    "\n",
    "    _ = dict()\n",
    "    _.update({'Наименование товара' : soup.find(\"div\", class_= \"op1-tt\").text})\n",
    "    _.update({'Cost': soup.find('div', class_='desc-short-prices').find('a', class_='ib').text})\n",
    "\n",
    "    for value in soup.find(\"table\", class_=\"one-col\").contents:\n",
    "\n",
    "        try:\n",
    "            _.update({value.find(\"span\", class_=\"gloss\").text : value.find(\"td\", class_=\"val\").text}) \n",
    "\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        \n",
    "    return _"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def create_json(urls: List[str]) -> Dict[str, List[Dict[str, str]]]:\n",
    "\n",
    "    global get_product_data, get_page_html\n",
    "\n",
    "    def data_list(url):\n",
    "        data = get_product_data(get_page_html(url))\n",
    "        data.update({'url' : url})\n",
    "        return data\n",
    "\n",
    "    return {'SSD': list(map(data_list,urls))}\n",
    "\n",
    "#json.dump(ssd, f, indent=4, ensure_ascii=False)"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "pages_urls = collect_product_pages(\n",
    "                                   data['URL'], get_product_amount_page(\n",
    "                                                                        data['URL'],\n",
    "                                                                        generate_headers(headers)\n",
    "                                                                        )\n",
    "                                   )"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "all_url_product = list(\n",
    "                       chain(\n",
    "                             *list(\n",
    "                                   map(\n",
    "                                       collect_url_from_page, pages_urls\n",
    "                                       )\n",
    "                                   )\n",
    "                             )\n",
    "                       ) # Распоковка всех списков"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "full_url_path_to_product = list(\n",
    "                                map(\n",
    "                                    lambda x: data['DOMAIN'] + x, all_url_product\n",
    "                                    )\n",
    "                                )"
   ],
   "execution_count": 0,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data_ssd = create_json(full_url_path_to_product)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "AttributeError: 'NoneType' object has no attribute 'text'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 1 in <module>",
      "    at line 10 in create_json",
      "    at line 6 in data_list",
      "    at line 6 in get_product_data",
      "AttributeError: 'NoneType' object has no attribute 'text'"
     ],
     "output_type": "error"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "with open('ssd_data.json', mode='a', encoding='UTF8') as f:\n",
    "    json.dump(data_ssd, f, indent=4, ensure_ascii=False)\n",
    "    f.close()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "NameError: name 'data_ssd' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 2 in <module>",
      "NameError: name 'data_ssd' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# def get_product_price(soup: BeautifulSoup) -> IO[Dict[str, str]]:\n",
    "#     '''Получает BeautifulSoup объект. Получает имя товара, и таблицу с характеристиками'''\n",
    "#     global get_page_html\n",
    "\n",
    "#     _ = dict()\n",
    "#     _.update({'Наименование товара' : soup.find(\"div\", class_= \"op1-tt\").text})\n",
    "    \n",
    "#     more_price = get_page_html(soup.find_all('div',class_='list-more-div-small blue-button h')[0].get('jsource'))\n",
    "#     market_list_full = soup.find_all('a',class_='it-shop') + more_price.find_all('a', class_= r\"\\'it-shop\\'\")\n",
    "#     price_product_list_full = soup.find('table',class_='where-buy-table').find_all('a') + \\\n",
    "#     more_price.find('table',class_=r\"\\'where-buy-table\").find_all('a',class_ = not 'yel-but-2')\n",
    "\n",
    "#     for key, value in zip(market_list_full,price_product_list_full):\n",
    "\n",
    "#         try:\n",
    "#             _.update({key.text: value.text}) \n",
    "\n",
    "#         except AttributeError:\n",
    "#             continue\n",
    "\n",
    "#     # else:\n",
    "#     #     print(f'Нужная таблица не найдена или пуста.')\n",
    "\n",
    "#     return _"
   ],
   "execution_count": 53,
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-a2bce8e3",
   "language": "python",
   "display_name": "PyCharm (HDD_SSD_Analayze)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}